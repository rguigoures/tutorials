{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec using Pytorch\n",
    "\n",
    "This notebook introduces how to implement the NLP technique, so-called word2vec, using Pytorch. This tutorial explains:\n",
    "1. how to generate the dataset\n",
    "2. how to build the neural network\n",
    "3. how to sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "Let's introduce the basic concepts:\n",
    "\n",
    "- *Corpus*: the corpus is the set of texts that define the data set\n",
    "- *vocabulary*: the set of words in the data set.\n",
    "\n",
    "For the example, we use the [Reuters dataset](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html), also available on sklearn and nltk. Non letter characters are removed from the string. Also the text is also set in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/rguigoures/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for text_id in reuters.fileids()[:500]:\n",
    "    text = reuters.raw(text_id)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"&lt;[a-z\\- ]+>\", '', text)\n",
    "    text = re.sub('[^a-z\\- ]+', '', text)\n",
    "    corpus.append([w for w in text.split(' ') if w != ''])\n",
    "    \n",
    "vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a bag of words approach. For each word of the data set, we need to extract the context words, i.e the neighboring words in a certain window of fixed length. For example, in the following sentence:\n",
    "\n",
    "*My cat is lazy, he sleeps all day long*\n",
    "\n",
    "For the target word *lazy*, if we consider a window of size 2, then context words are *cat*, *is*, *he* and *sleeps*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "context_tuple_list = []\n",
    "w = 2\n",
    "\n",
    "for text in corpus:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural network\n",
    "\n",
    "There two approach of word2vec:\n",
    "\n",
    "- CBOW (Continuous Bag Of Words). It predicts the target word conditionally to the context. In other words, context words are the input and the target word is the output.\n",
    "- Skip-gram. It predicts the context conditionally to the target word. In other words, the target word is the input and context words are the output.\n",
    "\n",
    "The following code is suited for CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context_word):\n",
    "        emb = self.embeddings(context_word)\n",
    "        hidden = self.linear(emb)\n",
    "        out = F.log_softmax(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_gain=0.01):\n",
    "        self.patience = patience\n",
    "        self.loss_list = [0]\n",
    "        self.min_gain = min_gain\n",
    "        \n",
    "    def update_loss(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "        if len(self.loss_list) > self.patience:\n",
    "            del self.loss_list[0]\n",
    "    \n",
    "    def stop_training(self):\n",
    "        if max(self.loss_list) - min(self.loss_list) < self.min_gain:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/jupyter_env3/lib/python3.6/site-packages/ipykernel/__main__.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "net = Word2Vec(embedding_size=2, vocab_size=vocabulary_size)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping()\n",
    "context_tensor_list = []\n",
    "\n",
    "for target, context in context_tuple_list:\n",
    "    target_tensor = autograd.Variable(torch.LongTensor([word_to_index[target]]))\n",
    "    context_tensor = autograd.Variable(torch.LongTensor([word_to_index[context]]))\n",
    "    context_tensor_list.append((target_tensor, context_tensor))\n",
    "    \n",
    "while True:\n",
    "    losses = []\n",
    "    for target_tensor, context_tensor in context_tensor_list:\n",
    "        net.zero_grad()\n",
    "        log_probs = net(context_tensor)\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "    print(np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_closest_word(word):\n",
    "    word_distance = []\n",
    "    emb = net.embeddings\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = word_to_index[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(vocabulary)):\n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j)\n",
    "            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_closest_word('france')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter_env3]",
   "language": "python",
   "name": "conda-env-jupyter_env3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
