{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec using Pytorch\n",
    "\n",
    "This notebook introduces how to implement the NLP technique, so-called word2vec, using Pytorch. This tutorial explains:\n",
    "1. how to generate the dataset\n",
    "2. how to build the neural network\n",
    "3. how to sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "Let's introduce the basic concepts:\n",
    "\n",
    "- *Corpus*: the corpus is the set of texts that define the data set\n",
    "- *vocabulary*: the set of words in the data set.\n",
    "\n",
    "For the example, we use the news from the Brown dataset, available on nltk. Non letter characters are removed from the string. Also the text is set in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/rguigoures/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for cat in ['romance']:\n",
    "    for text_id in brown.fileids(cat):\n",
    "        raw_text = list(itertools.chain.from_iterable(brown.sents(text_id)))\n",
    "        text = ' '.join(raw_text)\n",
    "        text = text.lower()\n",
    "        text.replace('\\n', ' ')\n",
    "        text = re.sub('[^a-z ]+', '', text)\n",
    "        corpus.append([w for w in text.split() if w != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in data preprocessing consists in subsampling frequent words, so that those words are not too often in the neighborhood of the target words. \n",
    "Let's call $p_i$ the proportion of word $i$ in the corpus. Then the probability $P(w_i)$ of keeping the word in the corpus is defined as follows:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\dfrac{10^{-3}}{p_i}\\left(\\sqrt{10^3 p_i} + 1\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random, math\n",
    "\n",
    "def subsample_frequent_words(corpus):\n",
    "    filtered_corpus = []\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "    sum_word_counts = sum(list(word_counts.values()))\n",
    "    word_counts = {word: word_counts[word]/float(sum_word_counts) for word in word_counts}\n",
    "    for text in corpus:\n",
    "        filtered_corpus.append([])\n",
    "        for word in text:\n",
    "            r  = random.random()\n",
    "            if r < (1+math.sqrt(word_counts[word] * 1e3)) * 1e-3 / float(word_counts[word]):\n",
    "                filtered_corpus[-1].append(word)\n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = subsample_frequent_words(corpus)\n",
    "vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a bag of words approach. For each word of the data set, we need to extract the context words, i.e the neighboring words in a certain window of fixed length. For example, in the following sentence:\n",
    "\n",
    "*My cat is lazy, he sleeps all day long*\n",
    "\n",
    "For the target word *lazy*, if we consider a window of size 2, then context words are *cat*, *is*, *he* and *sleeps*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 130669 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "context_tuple_list = []\n",
    "w = 2\n",
    "\n",
    "for text in corpus:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j]))\n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have built the data for the positive examples, i.e the words in the neighborhood of the target word, we need to build a data set with negative examples. The reason for that will be discussed later, when the model will be introduced. For each word in the corpus, the probability of sampling a negative context word is defined as follows:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\dfrac{\\mid w_i \\mid^{\\frac{3}{4}}}{\\displaystyle\\sum_{j=1}^n\\mid w_j \\mid^{\\frac{3}{4}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174380 negative examples have been generated.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import multinomial\n",
    "\n",
    "negative_tuple_list = []\n",
    "sample_probability = {}\n",
    "word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
    "for word in word_counts:\n",
    "    sample_probability[word] = word_counts[word]**0.75 / normalizing_factor\n",
    "for word in word_counts:\n",
    "    sampled_words = multinomial(4*word_counts[word], list(sample_probability.values()))\n",
    "    for idx, neg_word_count in enumerate(sampled_words):\n",
    "        for i in range(neg_word_count):\n",
    "            negative_tuple_list.append((word, list(word_counts.keys())[idx]))\n",
    "print(\"{} negative examples have been generated.\".format(len(negative_tuple_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural network\n",
    "\n",
    "There two approach of word2vec:\n",
    "\n",
    "- CBOW (Continuous Bag Of Words). It predicts the target word conditionally to the context. In other words, context words are the input and the target word is the output.\n",
    "- Skip-gram. It predicts the context conditionally to the target word. In other words, the target word is the input and context words are the output.\n",
    "\n",
    "The following code is suited for CBOW. \n",
    "\n",
    "### Negative examples\n",
    "\n",
    "Note that the network expects negative examples. The default word2vec algorithm exploits only positive examples and the output function is a softmax. However, using a softmax slows down the learning: softmax is normalized over all the vocabulary, then all the weights of the network are updated at each iteration. Consequently we decide using a sigmoid function as an output instead: only the weights involving the target word are updated. But then the network does not learn from negative examples anymore. That's why we need to input artificially generated negative examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context_word, neg=False):\n",
    "        emb = self.embeddings(context_word)\n",
    "        hidden = self.linear(emb)\n",
    "        if neg:\n",
    "            out = F.logsigmoid(-hidden)\n",
    "        else:\n",
    "            out = F.logsigmoid(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up the learning, we propose to use batches. This implies that a bunch of observations are forwarded through the network before doing the backpropagation. Besides being faster, this is also a good way to regularize the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    random.shuffle(context_tuple_list)\n",
    "    batches = []\n",
    "    batch_X, batch_Y = [], []\n",
    "    for i in range(len(context_tuple_list)):\n",
    "        batch_X.append(word_to_index[context_tuple_list[i][0]])\n",
    "        batch_Y.append(word_to_index[context_tuple_list[i][1]])\n",
    "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1:\n",
    "            tensor_X = autograd.Variable(torch.from_numpy(np.array(batch_X)).long())\n",
    "            tensor_Y = autograd.Variable(torch.from_numpy(np.array(batch_Y)).long())\n",
    "            batches.append((tensor_X, tensor_Y))\n",
    "            batch_X, batch_Y = [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping aims at stopping learning when the loss does not decrease significantly (min_gain parameter) anymore after a certain number of iterations (patience parameter). Early stopping is usually used on the validation loss, but in the case of word2vec, there is no validation since the approach is unsupervised. We apply early stopping on training loss instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_gain=0.01):\n",
    "        self.patience = patience\n",
    "        self.loss_list = [0]\n",
    "        self.min_gain = min_gain\n",
    "        \n",
    "    def update_loss(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "        if len(self.loss_list) > self.patience:\n",
    "            del self.loss_list[0]\n",
    "    \n",
    "    def stop_training(self):\n",
    "        if max(self.loss_list) - min(self.loss_list) < self.min_gain:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 420.000 pairs of words. The neural network in trained with the following parameters:\n",
    "\n",
    "- embedding size: 200\n",
    "- batch size: 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.989246\n",
      "17.797407\n",
      "17.666664\n",
      "17.56384\n",
      "17.475521\n",
      "17.393019\n",
      "17.302975\n",
      "17.190355\n",
      "17.042719\n",
      "16.857033\n",
      "16.636396\n",
      "16.394358\n",
      "16.148592\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "net = Word2Vec(embedding_size=200, vocab_size=vocabulary_size)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping(patience=5, min_gain=0.001)\n",
    "context_tensor_list = []\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
    "    negative_tuple_batches = get_batches(negative_tuple_list, batch_size=2000)\n",
    "    for _ in range(len(context_tuple_batches)):\n",
    "        net.zero_grad()\n",
    "        target_tensor, context_tensor = context_tuple_batches[i]\n",
    "        log_probs = net(context_tensor)\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "        target_tensor, negative_tensor = negative_tuple_batches[i]\n",
    "        log_probs = net(negative_tensor, neg=True)\n",
    "        loss += loss_function(log_probs, target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "    print(np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network trained, we can use the word embedding and compute the similarity between words. The following function computes the top n closest words for a given word. The similarity used is the cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = net.embeddings\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = word_to_index[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(vocabulary)):\n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j)\n",
    "            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest_word(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter_env3]",
   "language": "python",
   "name": "conda-env-jupyter_env3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
